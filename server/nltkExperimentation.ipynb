{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltkExperimentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDPeG5LUQpnW",
        "outputId": "d6d5ef6e-8740-4ed1-c241-eaedfc3b5838"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0kb71j3XEJ9"
      },
      "source": [
        "def dump(obj):\n",
        "  for attr in dir(obj):\n",
        "    print(\"obj.%s = %r\" % (attr, getattr(obj, attr)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n3fJCcYQVNy",
        "outputId": "53918108-d951-4915-b333-276e2aa367b0"
      },
      "source": [
        "from pprint import pprint\n",
        "from nltk.corpus import wordnet\n",
        "words = wordnet.synsets('large');\n",
        "\n",
        "print(words)\n",
        "# for lemma in words[0].lemmas():\n",
        "#             if lemma.antonyms():\n",
        "#                 for ant in lemma.antonyms():\n",
        "#                   print(ant.name())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('large.n.01'), Synset('large.a.01'), Synset('large.s.02'), Synset('bombastic.s.01'), Synset('big.s.11'), Synset('big.s.05'), Synset('large.s.06'), Synset('big.s.13'), Synset('large.r.01'), Synset('large.r.02'), Synset('boastfully.r.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnpAxLBiTs-s",
        "outputId": "5f9d0e66-c3bc-4007-ebb3-b1490b81a939"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "print(\"dies :\", lemmatizer.lemmatize(\"dying\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dies : dying\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAzAL3AeKf3h"
      },
      "source": [
        "import requests\n",
        "import tarfile\n",
        "\n",
        "url = \"https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\"\n",
        "response = requests.get(url, stream=True)\n",
        "file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
        "file.extractall(path=\".\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XSqmHtDqRt5y",
        "outputId": "508e3028-a3f8-4f39-ca0d-e1baecfaba56"
      },
      "source": [
        "!pip install sense2vec\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "# wordnet\n",
        "!pip install spacy-wordnet\n",
        "!python -m nltk.downloader wordnet\n",
        "!python -m nltk.downloader omw"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sense2vec\n",
            "  Downloading sense2vec-2.0.0-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (0.8.2)\n",
            "Collecting spacy<4.0.0,>=3.0.0\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 18.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from sense2vec) (4.8.2)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->sense2vec) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->sense2vec) (3.10.0.2)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 70.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.23.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 77.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.0.6)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (21.3)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.6)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (4.62.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.0->sense2vec) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->sense2vec) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, sense2vec\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 sense2vec-2.0.0 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-60.2.0-py3-none-any.whl (953 kB)\n",
            "\u001b[K     |████████████████████████████████| 953 kB 67.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.37.0\n",
            "    Uninstalling wheel-0.37.0:\n",
            "      Successfully uninstalled wheel-0.37.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-21.3.1 setuptools-60.2.0 wheel-0.37.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (60.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "     |████████████████████████████████| 13.9 MB 12.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (60.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting spacy-wordnet\n",
            "  Downloading spacy_wordnet-0.0.5-py2.py3-none-any.whl (650 kB)\n",
            "     |████████████████████████████████| 650 kB 12.6 MB/s            \n",
            "\u001b[?25hCollecting nltk<3.4,>=3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "     |████████████████████████████████| 1.4 MB 49.4 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394486 sha256=129b50288f7c0c193daded33cf16496ab95a9c3b15a22f7741aaa04f1726acf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk, spacy-wordnet\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3 spacy-wordnet-0.0.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKcWVant4wgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad548892-bf44-407e-edbd-df6b2545538d"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "s2v = nlp.add_pipe(\"sense2vec\")\n",
        "s2v.from_disk(\"s2v_old/\")\n",
        "\n",
        "\n",
        "from spacy import Language\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
        "\n",
        "spacy_wordnet_annotator = WordnetAnnotator(nlp.lang)\n",
        "@Language.component(\"wordnet\")\n",
        "def spacy_wordnet_wrapper(doc):\n",
        "    return spacy_wordnet_annotator(doc)\n",
        "\n",
        "nlp.add_pipe(\"wordnet\", after='tagger')\n",
        "\n",
        "doc = nlp(\"I was eating an apple in the morning on 12 april\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)\n",
        "  print(token._.wordnet.synsets())\n",
        "  print(token._.wordnet.lemmas())\n",
        "\n",
        "  # And automatically tags with wordnet domains\n",
        "  print(token._.wordnet.wordnet_domains())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 april 40 48 DATE\n",
            "I I PRON PRP nsubj X True True\n",
            "[Synset('iodine.n.01'), Synset('one.n.01'), Synset('i.n.03'), Synset('one.s.01')]\n",
            "[Lemma('iodine.n.01.iodine'), Lemma('iodine.n.01.iodin'), Lemma('iodine.n.01.I'), Lemma('iodine.n.01.atomic_number_53'), Lemma('one.n.01.one'), Lemma('one.n.01.1'), Lemma('one.n.01.I'), Lemma('one.n.01.ace'), Lemma('one.n.01.single'), Lemma('one.n.01.unity'), Lemma('i.n.03.I'), Lemma('i.n.03.i'), Lemma('one.s.01.one'), Lemma('one.s.01.1'), Lemma('one.s.01.i'), Lemma('one.s.01.ane')]\n",
            "['pharmacy', 'chemistry', 'physics', 'applied_science', 'photography', 'number', 'philosophy']\n",
            "was be AUX VBD aux xxx True True\n",
            "[Synset('washington.n.02'), Synset('be.v.01'), Synset('be.v.02'), Synset('be.v.03'), Synset('exist.v.01'), Synset('be.v.05'), Synset('equal.v.01'), Synset('constitute.v.01'), Synset('be.v.08'), Synset('embody.v.02'), Synset('be.v.10'), Synset('be.v.11'), Synset('be.v.12'), Synset('cost.v.01')]\n",
            "[Lemma('washington.n.02.Washington'), Lemma('washington.n.02.Evergreen_State'), Lemma('washington.n.02.WA'), Lemma('be.v.01.be'), Lemma('be.v.02.be'), Lemma('be.v.03.be'), Lemma('exist.v.01.exist'), Lemma('exist.v.01.be'), Lemma('be.v.05.be'), Lemma('equal.v.01.equal'), Lemma('equal.v.01.be'), Lemma('constitute.v.01.constitute'), Lemma('constitute.v.01.represent'), Lemma('constitute.v.01.make_up'), Lemma('constitute.v.01.comprise'), Lemma('constitute.v.01.be'), Lemma('be.v.08.be'), Lemma('be.v.08.follow'), Lemma('embody.v.02.embody'), Lemma('embody.v.02.be'), Lemma('embody.v.02.personify'), Lemma('be.v.10.be'), Lemma('be.v.11.be'), Lemma('be.v.11.live'), Lemma('be.v.12.be'), Lemma('cost.v.01.cost'), Lemma('cost.v.01.be')]\n",
            "['geology', 'geography', 'town_planning', 'administration', 'university', 'linguistics', 'tourism', 'nautical', 'book_keeping', 'factotum', 'meteorology', 'betting', 'sociology', 'environment', 'skiing', 'philosophy', 'social', 'psychological_features', 'home', 'card', 'psychoanalysis', 'finance', 'geometry', 'boxing', 'commerce', 'theatre', 'ethnology', 'philology', 'psychology', 'mathematics', 'time_period', 'pure_science', 'computer_science', 'economy', 'bowling', 'sexuality', 'quality', 'paranormal', 'telecommunication', 'statistics', 'occultism', 'exchange', 'roman_catholic', 'health', 'folklore', 'statistics', 'book_keeping', 'administration', 'factotum', 'agriculture', 'electrotechnology', 'telephony', 'gastronomy', 'social', 'card', 'music', 'biology', 'astrology', 'baseball', 'veterinary', 'drawing', 'telegraphy', 'time_period', 'furniture', 'food', 'dance', 'chemistry', 'bowling', 'anthropology', 'philately', 'grammar', 'oceanography', 'radio', 'exchange', 'color', 'architecture', 'numismatics', 'physics', 'metrology', 'insurance', 'engineering', 'publishing', 'psychological_features', 'hockey', 'animals', 'geometry', 'law', 'topography', 'tourism', 'ethnology', 'philology', 'geography', 'town_planning', 'religion', 'industry', 'art', 'medicine', 'graphic_arts', 'chess', 'cinema', 'paranormal', 'history', 'jewellery', 'telecommunication', 'radiology', 'statistics', 'sculpture', 'buildings', 'plastic_arts', 'sociology', 'philosophy', 'school', 'money', 'psychoanalysis', 'finance', 'university', 'play', 'genetics', 'theatre', 'mathematics', 'biochemistry', 'animal_husbandry', 'pure_science', 'economy', 'archaeology', 'electronics', 'sport', 'earth', 'sub', 'literature', 'astronautics', 'pedagogy', 'astronomy', 'psychiatry', 'pharmacy', 'meteorology', 'heraldry', 'politics', 'banking', 'anatomy', 'environment', 'geology', 'humanities', 'tennis', 'plants', 'number', 'linguistics', 'commerce', 'photography', 'theology', 'enterprise', 'psychology', 'gas', 'tv', 'computer_science', 'electricity', 'surgery', 'folklore', 'atomic_physic', 'optics', 'quality', 'free_time', 'physiology', 'acoustics', 'applied_science', 'roman_catholic', 'hunting', 'artisanship', 'rowing', 'veterinary', 'cinema', 'theatre', 'grammar', 'economy', 'commerce']\n",
            "eating eat VERB VBG ROOT xxxx True False\n",
            "[Synset('eating.n.01'), Synset('eat.v.01'), Synset('eat.v.02'), Synset('feed.v.06'), Synset('eat.v.04'), Synset('consume.v.05'), Synset('corrode.v.01')]\n",
            "[Lemma('eating.n.01.eating'), Lemma('eating.n.01.feeding'), Lemma('eat.v.01.eat'), Lemma('eat.v.02.eat'), Lemma('feed.v.06.feed'), Lemma('feed.v.06.eat'), Lemma('eat.v.04.eat'), Lemma('eat.v.04.eat_on'), Lemma('consume.v.05.consume'), Lemma('consume.v.05.eat_up'), Lemma('consume.v.05.use_up'), Lemma('consume.v.05.eat'), Lemma('consume.v.05.deplete'), Lemma('consume.v.05.exhaust'), Lemma('consume.v.05.run_through'), Lemma('consume.v.05.wipe_out'), Lemma('corrode.v.01.corrode'), Lemma('corrode.v.01.eat'), Lemma('corrode.v.01.rust')]\n",
            "['gastronomy', 'free_time', 'physiology', 'animal_husbandry', 'physiology', 'furniture', 'animal_husbandry', 'food', 'gastronomy', 'commerce', 'hunting', 'buildings', 'home', 'free_time', 'furniture', 'physiology', 'animal_husbandry', 'psychiatry', 'food', 'gastronomy', 'railway', 'tourism', 'buildings', 'agriculture', 'pharmacy', 'gastronomy', 'fishing', 'environment', 'anatomy', 'geology', 'home', 'athletics', 'psychoanalysis', 'plants', 'biology', 'animals', 'entomology', 'veterinary', 'diving', 'dentistry', 'furniture', 'biochemistry', 'animal_husbandry', 'industry', 'food', 'chemistry', 'medicine', 'folklore', 'surgery', 'archaeology', 'physiology', 'swimming', 'hydraulics', 'jewellery', 'mountaineering', 'oceanography', 'color', 'hunting', 'psychological_features', 'economy', 'fishing', 'chemistry', 'earth']\n",
            "an an DET DT det xx True True\n",
            "[Synset('associate_in_nursing.n.01')]\n",
            "[Lemma('associate_in_nursing.n.01.Associate_in_Nursing'), Lemma('associate_in_nursing.n.01.AN')]\n",
            "['university']\n",
            "apple apple NOUN NN dobj xxxx True False\n",
            "[Synset('apple.n.01'), Synset('apple.n.02')]\n",
            "[Lemma('apple.n.01.apple'), Lemma('apple.n.02.apple'), Lemma('apple.n.02.orchard_apple_tree'), Lemma('apple.n.02.Malus_pumila')]\n",
            "['mythology', 'archery', 'plants', 'food', 'chemistry', 'entomology', 'gastronomy', 'agriculture', 'biology', 'plants']\n",
            "in in ADP IN prep xx True True\n",
            "[Synset('inch.n.01'), Synset('indium.n.01'), Synset('indiana.n.01'), Synset('in.s.01'), Synset('in.s.02'), Synset('in.s.03'), Synset('in.r.01')]\n",
            "[Lemma('inch.n.01.inch'), Lemma('inch.n.01.in'), Lemma('indium.n.01.indium'), Lemma('indium.n.01.In'), Lemma('indium.n.01.atomic_number_49'), Lemma('indiana.n.01.Indiana'), Lemma('indiana.n.01.Hoosier_State'), Lemma('indiana.n.01.IN'), Lemma('in.s.01.in'), Lemma('in.s.02.in'), Lemma('in.s.03.in'), Lemma('in.r.01.in'), Lemma('in.r.01.inwards'), Lemma('in.r.01.inward')]\n",
            "['free_time', 'archery', 'pure_science', 'hockey', 'body_care', 'geometry', 'metrology', 'graphic_arts', 'transport', 'photography', 'chemistry', 'administration', 'geography', 'town_planning', 'geometry']\n",
            "the the DET DT det xxx True True\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "morning morning NOUN NN pobj xxxx True False\n",
            "[Synset('morning.n.01'), Synset('good_morning.n.01'), Synset('dawn.n.01'), Synset('dawn.n.02')]\n",
            "[Lemma('morning.n.01.morning'), Lemma('morning.n.01.morn'), Lemma('morning.n.01.morning_time'), Lemma('morning.n.01.forenoon'), Lemma('good_morning.n.01.good_morning'), Lemma('good_morning.n.01.morning'), Lemma('dawn.n.01.dawn'), Lemma('dawn.n.01.dawning'), Lemma('dawn.n.01.morning'), Lemma('dawn.n.01.aurora'), Lemma('dawn.n.01.first_light'), Lemma('dawn.n.01.daybreak'), Lemma('dawn.n.01.break_of_day'), Lemma('dawn.n.01.break_of_the_day'), Lemma('dawn.n.01.dayspring'), Lemma('dawn.n.01.sunrise'), Lemma('dawn.n.01.sunup'), Lemma('dawn.n.01.cockcrow'), Lemma('dawn.n.02.dawn'), Lemma('dawn.n.02.morning')]\n",
            "['free_time', 'time_period', 'mythology', 'time_period', 'religion', 'astronomy', 'astrology']\n",
            "on on ADP IN prep xx True True\n",
            "[Synset('on.a.01'), Synset('on.a.02'), Synset('along.r.01'), Synset('on.r.02'), Synset('on.r.03')]\n",
            "[Lemma('on.a.01.on'), Lemma('on.a.02.on'), Lemma('along.r.01.along'), Lemma('along.r.01.on'), Lemma('on.r.02.on'), Lemma('on.r.03.on')]\n",
            "['electrotechnology']\n",
            "12 12 NUM CD nummod dd False False\n",
            "[Synset('twelve.n.01'), Synset('twelve.s.01')]\n",
            "[Lemma('twelve.n.01.twelve'), Lemma('twelve.n.01.12'), Lemma('twelve.n.01.XII'), Lemma('twelve.n.01.dozen'), Lemma('twelve.s.01.twelve'), Lemma('twelve.s.01.12'), Lemma('twelve.s.01.xii'), Lemma('twelve.s.01.dozen')]\n",
            "['number']\n",
            "april april PROPN NNP pobj xxxx True False\n",
            "[Synset('april.n.01')]\n",
            "[Lemma('april.n.01.April'), Lemma('april.n.01.Apr')]\n",
            "['free_time', 'time_period', 'play', 'astrology', 'metrology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlP5m_9qVNjI",
        "outputId": "1a3bcfec-67ff-4ebd-db0a-b95c06fcfbd6"
      },
      "source": [
        "doc = nlp(\"A sentence about natural language processing.\")\n",
        "print(doc.cats)\n",
        "most_similar = doc[3:6]._.s2v_most_similar(3)\n",
        "print(most_similar)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "[(('machine learning', 'NOUN'), 0.8987), (('computer vision', 'NOUN'), 0.8636), (('deep learning', 'NOUN'), 0.8573)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ces2H27tkZS0"
      },
      "source": [
        "doc = nlp(\"I was eating an apple\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    if ent._.in_s2v:\n",
        "      print(ent.text, ent.start_char, ent.end_char, ent.label_, ent._.s2v_most_similar(3))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQdWirUIM6dQ"
      },
      "source": [
        "import random\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "PRIOR_LIST = ['NUM', 'ADJ']\n",
        "\n",
        "def _getAntonyms(token):\n",
        "    antonyms = []\n",
        "    for sysnet in token._.wordnet.synsets():\n",
        "        for lemma in sysnet.lemmas():\n",
        "            if lemma.antonyms():\n",
        "                for ant in lemma.antonyms():\n",
        "                    antonyms.append(ant.name())\n",
        "    return antonyms\n",
        "\n",
        "\n",
        "def _getNumOptions(token):\n",
        "    options = []\n",
        "    for i in range(5):\n",
        "      rep = re.sub('d', lambda x: str(random.randint(0,9)), token.shape_)\n",
        "      options.append(rep)\n",
        "    return options\n",
        "\n",
        "\n",
        "def _forcedOptions(token):\n",
        "    options = []\n",
        "    for sysnet in token._.wordnet.synsets():\n",
        "        for hyper in sysnet.hypernyms():\n",
        "          for item in hyper.hyponyms():\n",
        "                for term in item.lemmas():\n",
        "                    name = term.name()\n",
        "                    if name == token.text or name == token.lemma_:\n",
        "                        continue\n",
        "                    name = name.replace(\"_\", \" \")\n",
        "                    if name is not None and name not in options:\n",
        "                      options.append(name)\n",
        "\n",
        "    return options\n",
        "\n",
        "def _mispelledOptions(token):\n",
        "    options = []\n",
        "    for i in range(6):\n",
        "      ind = random.randint(0,len(token.text)-1)\n",
        "      rep = token.text[:ind] + random.choice(string.ascii_letters) + token.text[ind+1:]\n",
        "      if(token.text is not rep):\n",
        "        options.append(rep)\n",
        "    return options\n",
        "\n",
        "\n",
        "def _createOptions(token):\n",
        " \n",
        "  senseWords = token._.s2v_most_similar(20)\n",
        "  # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "  #                     token.shape_, token.is_alpha, token.is_stop, senseWords)\n",
        "  filteredSense = dict()\n",
        "  for sense in senseWords:\n",
        "    # print(token.lemma_, \"===>\", sense[0][0])\n",
        "    avoidWord = ps.stem(token.lemma_)\n",
        "    reWord = ''\n",
        "    reKey = ''\n",
        "    for word in sense[0][0].split():\n",
        "      stemmed = ps.stem(word)\n",
        "      if avoidWord == stemmed:\n",
        "        reWord = ''\n",
        "        reKey = ''\n",
        "        break;\n",
        "      else: \n",
        "        reKey = reKey + ' ' + stemmed\n",
        "        reWord = reWord  + ' ' + word\n",
        "\n",
        "    if reWord:\n",
        "      filteredSense[reKey.strip()] = reWord.strip().rstrip('-')\n",
        "  options =  list(filteredSense.values())\n",
        "    \n",
        "  return options\n",
        "  \n",
        "def getOptions(item):\n",
        "    print('\\n\\n')\n",
        "    context = item['context']\n",
        "    answer = item['answer']#[6:].strip()\n",
        "    question = item['question']\n",
        "\n",
        "    doc = nlp(context)\n",
        "\n",
        "    options = []\n",
        "    for ent in doc.ents:\n",
        "      if re.search(ent.text, answer, re.IGNORECASE):\n",
        "            if ent._.in_s2v:\n",
        "               options = _createOptions(ent)\n",
        "               if(len(options) > 0):\n",
        "                   return {\n",
        "                      'answer': answer.replace(ent.text, '________'),\n",
        "                      'correct': ent.text,\n",
        "                      'options': options,\n",
        "                      'question': question\n",
        "                  }\n",
        "\n",
        "    if len(options) == 0:\n",
        "      tokenList = []\n",
        "      for token in doc:\n",
        "        if not token.is_punct and not token.is_stop and re.search(token.text, answer, re.IGNORECASE):\n",
        "          tokenList.append(token)\n",
        "      random.shuffle(tokenList)\n",
        "      tokenList.sort(key = lambda tok: PRIOR_LIST.index(tok.pos_) if tok.pos_ in PRIOR_LIST else 1000)\n",
        "\n",
        "\n",
        "      if (len(tokenList) > 0 and tokenList[0].pos_ == PRIOR_LIST[0]): # if num\n",
        "        token = tokenList[0]\n",
        "        options = _getNumOptions(token)\n",
        "        if(len(options) > 0):\n",
        "            return {\n",
        "                      'answer': answer.replace(token.text, '________'),\n",
        "                      'correct': token.text,\n",
        "                      'options': options,\n",
        "                      'question': question\n",
        "                  }\n",
        "\n",
        "      if len(options) == 0:\n",
        "        for token in tokenList: # random check options significantly\n",
        "          if token._.in_s2v:\n",
        "            options = _createOptions(token)\n",
        "            if(len(options) > 0):\n",
        "              return {\n",
        "                        'answer': answer.replace(token.text, '________'),\n",
        "                        'correct': token.text,\n",
        "                        'options': options,\n",
        "                        'question': question\n",
        "                    }\n",
        "\n",
        "      if len(options) == 0:\n",
        "        for token in tokenList:\n",
        "            options = _getAntonyms(token)\n",
        "            if(len(options) > 0):\n",
        "             return {\n",
        "                      'answer': answer.replace(token.text, '________'),\n",
        "                      'correct': token.text,\n",
        "                      'options': options,\n",
        "                      'question': question\n",
        "                  }\n",
        "\n",
        "      if len(options) == 0:\n",
        "        for token in tokenList:\n",
        "            options = _forcedOptions(token)\n",
        "            if(len(options) > 0):\n",
        "              return {\n",
        "                      'answer': answer.replace(token.text, '________'),\n",
        "                      'correct': token.text,\n",
        "                      'options': options,\n",
        "                      'question': question\n",
        "                  }\n",
        "    options = _mispelledOptions(token)\n",
        "    return {\n",
        "         'answer': answer,\n",
        "         'correct': token.text,\n",
        "         'options': options,\n",
        "         'question': question\n",
        "          }"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd_GAorhNA_z",
        "outputId": "c187508b-b6ae-4d8a-e035-943f99b12c9f"
      },
      "source": [
        "QUESTIONS = [\n",
        "             {'answer': '300 BC', 'question': 'When was the Arthashastra written?', 'context': 'The Arthashastra (written around 300 BC) advised the Mauryan government to reserve some forests for wild elephants for use in the army, and to execute anyone who killed them.'},\n",
        "  # {'answer': 'Indus Valley Civilization', 'question': 'When have elephants been working?', 'context': 'Elephants have been working animals since at least the Indus Valley Civilization and continue to be used in modern times.'},\n",
        "  # {'answer': '10–20 years old', 'question': 'When are elephants typically captured from the wild?', 'context': 'These animals are typically captured from the wild when they are 10–20 years old when they can be trained quickly and easily, and will have a longer working life.'},\n",
        "  # {'answer': '1950', 'question': 'Since what year have tranquillisers been used?', 'context': 'They were traditionally captured with traps and lassos, but since 1950, tranquillisers have been used.Individuals of the Asian species have been often trained as working animals.'},\n",
        "  # {'answer': 'coffee beans', 'question': 'What are the animals used to digest for Black Ivory coffee?', 'context': 'In northern Thailand, the animals are used to digest coffee beans for Black Ivory coffee.'},\n",
        "  # {'answer': 'mechanised tools', 'question': 'What are the elephants valued over?', 'context': 'They are valued over mechanised tools because they can work in relatively deep water, require relatively little maintenance, need only vegetation and water as fuel and can be trained to memorise specific tasks.'},\n",
        "  # {'answer': 'over 30', 'question': 'How many commands can elephants be trained to respond to?', 'context': 'Elephants can be trained to respond to over 30 commands.'},\n",
        "  # {'answer': 'Musth bulls', 'question': 'What can be difficult and dangerous to work with and are chained and semi-starved until the condition passes?', 'context': 'Musth bulls can be difficult and dangerous to work with and are chained and semi-starved until the condition passes.'},\n",
        "  # {'answer': 'abuse', 'question': 'In India, many working elephants are alleged to have been subject to what?', 'context': 'In India, many working elephants are alleged to have been subject to abuse.'},\n",
        "  # {'answer': 'The Prevention of Cruelty to Animals Act of 1960', 'question': 'What act protects elephants from abuse?', 'context': 'They and other captive elephants are thus protected under The Prevention of Cruelty to Animals Act of 1960.\\nIn both Myanmar and Thailand, deforestation and other economic factors have resulted in sizable populations of unemployed elephants resulting in health problems for the elephants themselves as well as economic and safety problems for the people amongst whom they live.The practice of working elephants has also been attempted in Africa.'},\n",
        "  # {'answer': 'Leopold II of Belgium', 'question': 'Who began the taming of African elephants in the Belgian Congo?', 'context': 'The taming of African elephants in the Belgian Congo began by decree of Leopold II of Belgium during the 19th century and continues to the present with the Api Elephant Domestication Centre.'},\n",
        "  # {'answer': '13,000–16,500', 'question': 'How many working elephants were employed in Asia in 2000?', 'context': 'There were 13,000–16,500 working elephants employed in Asia in 2000.'}\n",
        "]\n",
        "for ques in QUESTIONS:\n",
        "    options = getOptions(ques)\n",
        "    print(options)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "BC ORG\n",
            "{'answer': '300 ________', 'correct': 'BC', 'options': ['BC.', 'B.C.', 'bc', 'BC,', 'NB', 'Ontario', 'NS', 'Manitoba', 'CA', 'Alberta', 'British Columbia', 'RI', 'Saskatchewan'], 'question': 'When was the Arthashastra written?'}\n"
          ]
        }
      ]
    }
  ]
}